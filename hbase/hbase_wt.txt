# jdk 8 needs to be installed and JAVA_HOME set
# donwload hbase and extract it 
sudo tar xvzf <downloaded hbase tgz archive> -C /usr/lib
# link the hbase extracted folder under /usr/lib to /usr/lib/hbase
sudo ln -s /usr/lib/<extracted hbase folder> /usr/lib/hbase
# from comfigs copy hbase-site.xml and hbase-env.sh to /usr/lib/hbase/conf
# note in the hbase-site.xml the hbase and zookeeper data directories and hbase.distributed.cluster set to true
# in hdfs make the hbase directory
# create the zookeeper data directory
# create the local hbase directory
sudo mkdir /tmp/hbtmp
sudo mkdir /var/lib/hbzk
sudo hdfs dfs -mkdir /hbase

# check the hbase configuration
cat /usr/lib/hbase/conf/hbase-site.xml
# empty the zookeeper directory
sudo rm -rf /var/lib/hbzk/*
# start hbase
sudo /usr/lib/hbase/bin/start-hbase.sh
# start the shell
hbase shell
# check the status
status
# get detailed status
status 'detailed'
# get version
version
# check the tables available
list
# get help on tables
table_help
# create a table to play around with two column families
create 't1', 'f1', 'f2'
# describe the created table
describe 't1'
# Syntax:  put <'tablename'>,<'rowname'>,<'columnvalue'>,<'value'>
# put a row into the table without a quailfier
put 't1', 'r1', 'f1', 'value'
# put a row into the table with a family and qualifer
put 't1', 'r1', 'f1:c1', 'r1f1c1v1'

# put ten rows each with values for f1:c1 and f2:c1
for x in 1..9 do
put 't1', "r#{x}", 'f1:c1', "r#{x}f1c1v#{x}"
put 't1', "r#{x}", 'f1:c2', "r#{x}f1c1v#{x}"
end
# put ten rows in columns c1 and c2 for family f2
for x in 1..9 do
put 't1', "r#{x}", 'f2:c1', "r#{x}f2c1v#{x}"
put 't1', "r#{x}", 'f2:c2', "r#{x}f2c1v#{x}"
end
# exercise - add rows from r11 to r20 with each having values for f1 and f2 to t1

# retrieve data
# Syntax: get <'tablename'>, <'rowname'>, {< Additional parameters>}
get 't1', 'r1'
get 't1', 'r1', 'f1'
get 't1', 'r1', 'f1:c1'
get 't1', 'r1', {COLUMNS => ['f1','f2']}
get 't1', 'r1'
get 't1', 'r1', {COLUMN => 'f1:c1', VERSIONS => 5}

# delete cells
# Syntax:delete <'tablename'>,<'row name'>,<'column name'>
delete 't1', 'r1', 'f1:c1'

# to alter a table
# here increasing number of versions for the column family to 5
alter 't1', NAME=>'f1', VERSIONS => 5
# alter add on a column to the table
alter 't1', NAME=>'f3', VERSIONS => 3
# alter table multiple add two new column families
alter 't1', {NAME => 'f4'}, {NAME => 'f5'}
# alter table remove a column family
alter 't1', 'delete' => 'c3'
# You can also change table-scope attributes like MAX_FILESIZE, READONLY,
# MEMSTORE_FLUSHSIZE, NORMALIZATION_ENABLED, NORMALIZER_TARGET_REGION_COUNT,
# NORMALIZER_TARGET_REGION_SIZE(MB), DURABILITY, etc. These can be put at the # end;
# for example, to change the max size of a region to 128MB, do
alter 't1', MAX_FILESIZE => '134217728'
# remove table attribute
alter 't1', METHOD => 'table_att_unset', NAME => 'MAX_FILESIZE'
# count table rows
# default interval is 1000
count 't1', INTERVAL => 10000

# to drop a table, disable and then drop
is_enabled 't1'
disable 't1'
is_enabled 't1'
drop 't1'

# list the namespaces
list_namespace
# list the tables in a namespace
list_namespace_tables 'hbase'
# scan a namespace table
scan 'hbase:meta'
# look for regioninfo
scan 'hbase:meta', {FILTER => "QualifierFilter(=,'regexstring:regioninfo')"}
# list regions for a table
list_regions 't1'

# see the files that make up hbase on hdfs
hdfs dfs -ls /hbase
hdfs dfs -ls /hbase/WALs
-- there is a directory for each region server
#  see the oldWALs directory on hdfs
hdfs dfs -ls /hbase/oldWAL
hdfs dfs -ls /hbase/oldWAL | wc -l
# see the hbase id identifying the cluster
hdfs dfs -cat /hbase/hbase.id
# see the direcoties created for the new table
hdfs dfs -ls -R /hbase/data/default/t1

# counters
create 'counters', 'daily', 'weekly', 'monthly'
incr 'counters', '20230101', 'daily:hits', 1
incr 'counters', '20230101', 'daily:hits', 1
incr 'counters', '20230101', 'daily:hits', 1
incr 'counters', '20230101', 'daily:hits', 1

incr 'counters', '20230102', 'daily:hits', 1
incr 'counters', '20230102', 'daily:hits', 1

get_counter 'counters', '20230101', 'daily:hits'
get_counter 'counters', '20230102', 'daily:hits'

# install happybase
sudo pip3 install happybase
# start the thrift server
sudo /usr/lib/hbase/bin/hbase thrift
# then use a combination of the code  at 
https://github.com/samarkk/spark-python/blob/main/chbase/hbase_ops.py
python shell and hbase shell to explore filters

# list the regions of Table
list_regions 'mytable'
# split the regions
split 'mybable'
# merge the split regions to see the command in action
merge_region <region1name>, <region2name>
# for mergin non consecutive regions add a third argument true


